# -*- coding: utf-8 -*-
"""customer segmentation using K - means clustering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R9hBn_GF8UMlEMoUbDAX5V7EeZesLkq9

Importing the dependencies
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

"""Data collection and processing"""

#data loading into pandas data frame
customer_data = pd.read_csv('/content/Mall_Customers.csv')

#printing the first 5 rows in the dataframe
customer_data.head()

#printing the nubmer of rows and columns
customer_data.shape

#getting more infromation about dataset
customer_data.info()

#checking the number of null values in a row
customer_data.isnull().sum()

"""Choosing annual income and spending score column"""

#to print a list of values containing all rows and 3rd and 4th column
x = customer_data.iloc[:,[3,4]].values

print(x)

"""choosing the number of clusters

The Elbow Method is a popular technique used for this purpose in K-Means clustering to find the optimal value of k(clusters).

The Elbow Method helps us find this optimal k value.

- We iterate over a range of k values, typically from 1 to n (where n is a hyper-parameter you choose).
- For each k, we calculate the Within-Cluster Sum of Squares (WCSS).

- We calculate a distance measure called WCSS (Within-Cluster Sum of Squares). This tells us how spread out the data points are within each cluster.
- We try different k values (number of clusters). For each k, we run KMeans and calculate the WCSS.
- We plot a graph with k on the X-axis and WCSS on the Y-axis.
- Identifying the Elbow Point: As we increase kkk, the WCSS typically decreases - because we’re creating more clusters, which tend to capture more data variations. However, there comes a point where adding more clusters results in only a marginal decrease in WCSS. This is where we observe an “elbow” shape in the graph.


- Before the elbow: Increasing k significantly reduces WCSS, indicating that new clusters effectively capture more of the data’s variability.
- After the elbow: Adding more clusters results in a minimal reduction in WCSS, suggesting that these extra clusters may not be necessary and could lead to overfitting.

WCSS --> within clusters sum of squares
"""

#finding wcss value for different number of cluster

wcss=[] #inertia

for i in range(1, 11):
  kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)
  kmeans.fit(x)
  #kmeans_inertia calculates WCSS for the current state of the k-Means clustering model
  wcss.append(kmeans.inertia_)

print(wcss)

#plot a elbow graph
sns.set()
plt.plot(range(1,11),wcss)
plt.title('The Elbow point graph')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()

"""Optimum number of clusters = 5

training the k-means clustering model
"""

kmeans = KMeans(n_clusters=5, init='k-means++', random_state=0)

#return a label for each data point based on their cluster

y = kmeans.fit_predict(x)
print(y)

"""visiulaizing all the clusters

x[y==0, 0]: Selects the first feature (x-coordinate) of all data points that are assigned to Cluster 1 (where the cluster label is 0 in the array y).

x[y==0, 1]: Selects the second feature (y-coordinate) of those same data points.

s=50: Sets the size of the plotted points to 50. Larger values make the points bigger in the plot.
"""

#plotting all the clusters and their centroids
plt.figure(figsize=(8,8))
plt.scatter(x[y==0,0], x[y==0,1], s=50, c='green', label='cluster 1')

plt.scatter(x[y==1,0], x[y==1,1], s=50, c='red', label='cluster 2')

plt.scatter(x[y==2,0], x[y==2,1], s=50, c='yellow', label='cluster 3')

plt.scatter(x[y==3,0], x[y==3,1], s=50, c='violet', label='cluster 4')

plt.scatter(x[y==4,0], x[y==4,1], s=50, c='blue', label='cluster 5')

plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], s=100, c='black', label='Centroid')

plt.title("customer groups")
plt.xlabel('Annual Income')
plt.ylabel('Spending score')
plt.show()

